{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_WOqLp5x_Et"
   },
   "source": [
    "# Learning Process management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ue8WClJR9M_"
   },
   "source": [
    "### Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2I21IRjR9NA"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saWy9emWx_Ew"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCuMrwtfI4mC"
   },
   "outputs": [],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-hP2u8WTdwC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoKp8PbO6Lpj"
   },
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "!nvidia-smi -L\n",
    "!nvidia-smi\n",
    "!lscpu |grep 'Model name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ow-qDNR9NC"
   },
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45wfuLSqR9NC"
   },
   "outputs": [],
   "source": [
    "parameters = dict(learning_rate = [0.001],\n",
    "                  batch_size = [100],\n",
    "                  weight_decay = [0],\n",
    "                  epoch_number = [20],\n",
    "                  scheduler_step_size = [5],\n",
    "                  scheduler_gamma = [1]   )\n",
    "param_values = [v for v in parameters.values()]\n",
    "trg_dataset_ref = 1\n",
    "valloader_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om049J11R9NC"
   },
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnNRMcj_UteN"
   },
   "outputs": [],
   "source": [
    "# Retrieve normalisation parameters \n",
    "\n",
    "norm_param_df = pd.read_csv('/content/drive/MyDrive/KASHIKO/DATASET/TRG_DATASET_NORM_PARAM.csv')\n",
    "\n",
    "meanR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanR\"].item()\n",
    "meanG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanG\"].item()\n",
    "meanB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanB\"].item()\n",
    "\n",
    "stdR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdR\"].item()\n",
    "stdG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdG\"].item()\n",
    "stdB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdB\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGB01PCwRc6W"
   },
   "outputs": [],
   "source": [
    "# Prepare normalised dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/KASHIKO/DATASET/TRG_1_FINAL/',\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yp3aI37R9NE"
   },
   "source": [
    "### Define model + Optimizer + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2VV76PiJAf-"
   },
   "outputs": [],
   "source": [
    "# Model = 2 convolutional layers + 3 fully connected layers\n",
    "if trg_dataset_ref == 1:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "            self.bn1 = nn.BatchNorm2d(12)\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "            self.bn2 = nn.BatchNorm2d(24)\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(24*53*53, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = x.view(-1,24*53*53)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "elif trg_dataset_ref == 2:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "            nn.init.xavier_uniform_(self.conv1.weight)\n",
    "            nn.init.constant_(self.conv1.bias, 0.1)\n",
    "            self.bn1 = nn.BatchNorm2d(12)\n",
    "            nn.init.constant_(self.bn1.weight, 1)\n",
    "            nn.init.constant_(self.bn1.bias, 0)\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "            nn.init.xavier_uniform_(self.conv2.weight)\n",
    "            nn.init.constant_(self.conv2.bias, 0.1)\n",
    "            self.bn2 = nn.BatchNorm2d(24)\n",
    "            nn.init.constant_(self.bn2.weight, 1)\n",
    "            nn.init.constant_(self.bn2.bias, 0)\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(24*53*53, 120)\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.constant_(self.fc1.bias, 0.1)  \n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.constant_(self.fc2.bias, 0.1)\n",
    "            self.fc3 = nn.Linear(84, 2)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "            nn.init.constant_(self.fc3.bias, 0.1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = x.view(-1,24*53*53)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "elif trg_dataset_ref == 3:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "            nn.init.xavier_normal_(self.conv1.weight)\n",
    "            nn.init.constant_(self.conv1.bias, 0.1)\n",
    "            self.bn1 = nn.BatchNorm2d(12)\n",
    "            nn.init.constant_(self.bn1.weight, 1)\n",
    "            nn.init.constant_(self.bn1.bias, 0)\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "            nn.init.xavier_normal_(self.conv2.weight)\n",
    "            nn.init.constant_(self.conv2.bias, 0.1)\n",
    "            self.bn2 = nn.BatchNorm2d(24)\n",
    "            nn.init.constant_(self.bn2.weight, 1)\n",
    "            nn.init.constant_(self.bn2.bias, 0)\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)\n",
    "            self.fc1 = nn.Linear(24*53*53, 120)\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.constant_(self.fc1.bias, 0.1)  \n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "            nn.init.constant_(self.fc2.bias, 0.1)\n",
    "            self.fc3 = nn.Linear(84, 2)\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "            nn.init.constant_(self.fc3.bias, 0.1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = x.view(-1,24*53*53)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS2zYmi4JEJV"
   },
   "outputs": [],
   "source": [
    "# Define Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGQ8a79dR9NF"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAa8cEX9JJql"
   },
   "outputs": [],
   "source": [
    "for learning_rate, batch_size, weight_decay, epoch_number, scheduler_step_size, scheduler_gamma in product(*param_values): \n",
    "    \n",
    "    # Initialise model    \n",
    "    net = Net()    \n",
    "    \n",
    "    # Split dataset into a training dataset and a validation dataset\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - valloader_size, valloader_size])\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        drop_last=True)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        drop_last=True)\n",
    "\n",
    "    # Define Optimizer and scheduler\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    \n",
    "    # Initialize tensorboard SummaryWriter file/directory\n",
    "    date_now = str(date.today())\n",
    "    time_now = datetime.now().strftime(\"%H:%M:%S\")    \n",
    "    log_dir_root = os.path.join('/content/drive/My Drive/KASHIKO/RUNS', date_now + '_' + time_now + '_')\n",
    "    comment = f' trg_dataset{trg_dataset_ref} batch_size={batch_size} learning_rate={learning_rate} scheduler_step_size={scheduler_step_size} scheduler_gamma={scheduler_gamma} weight_decay={weight_decay} epoch_number={epoch_number}'\n",
    "    log_dir = log_dir_root + comment\n",
    "    tb = SummaryWriter(log_dir)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epoch_number):  # loop over the dataset multiple times\n",
    "\n",
    "        net.train()\n",
    "        net.requires_grad = True\n",
    "        trg_running_loss = 0.0\n",
    "        trg_epoch_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            trg_running_loss += loss.item()\n",
    "            trg_epoch_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, trg_running_loss / 10))\n",
    "                trg_running_loss = 0.0\n",
    "\n",
    "        # At the end of each epoch, check the performance of the network using the validation dataset\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        TP = 0.0\n",
    "        TN = 0.0\n",
    "        FP = 0.0\n",
    "        FN = 0.0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                net.eval()\n",
    "                images, labels = data\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                TP += (predicted == labels & labels == 0).sum().item()\n",
    "                TN += (predicted == labels & labels == 1).sum().item()\n",
    "                FP += (predicted != labels & predicted == 0).sum().item()\n",
    "                FN += (predicted != labels & predicted == 1).sum().item()\n",
    "                val_loss += criterion(outputs, labels)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_sensitivity_TPR = TP/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        val_specificity_TNR = TN/(TN+FP) if (TN+FP)!=0.0 else -1.0\n",
    "        val_FPR = FP/(TN+FP) if (TN+FP)!=0.0 else -1.0\n",
    "        val_FNR = FN/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        val_precision = TP/(TP+FP) if (TP+FP)!=0.0 else -1.0\n",
    "        val_recall = TP/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        inv_val_recall = 1/val_recall if val_recall!=0.0 else -1.0\n",
    "        inv_val_precision = 1/val_precision if val_precision!=0.0 else -1.0\n",
    "        val_F1_score = 2/(inv_val_precision + inv_val_recall) if (inv_val_precision + inv_val_recall)!=0.0 else -1.0\n",
    "        print(f'Accuracy of the network on the 1000 test images:{val_accuracy}')\n",
    "\n",
    "        # Store metrics and other parameters in tensorboard SummaryWriter\n",
    "        # Metrics\n",
    "        tb.add_scalar('Training Loss', trg_epoch_loss/(int((len(dataset)-valloader_size)/batch_size) * batch_size), epoch+1)\n",
    "        tb.add_scalar('Validation Loss', val_loss/valloader_size, epoch+1)\n",
    "        tb.add_scalar('Accuracy', val_accuracy, epoch+1)\n",
    "        tb.add_scalar('Sensitivity TPR', val_sensitivity_TPR, epoch+1)\n",
    "        tb.add_scalar('Specificity TNR', val_specificity_TNR, epoch+1)        \n",
    "        tb.add_scalar('FPR', val_FPR, epoch+1)\n",
    "        tb.add_scalar('FNR', val_FNR, epoch+1)        \n",
    "        tb.add_scalar('Precision', val_precision, epoch+1)\n",
    "        tb.add_scalar('Recall', val_recall, epoch+1)        \n",
    "        tb.add_scalar('F1 Score', val_F1_score, epoch+1)\n",
    "        # DEBUG\n",
    "        tb.add_scalar('False Positive', FP, epoch+1)        \n",
    "        tb.add_scalar('False Negative', FN, epoch+1)\n",
    "        tb.add_scalar('True Positive', TP, epoch+1)        \n",
    "        tb.add_scalar('True Negative', TN, epoch+1)\n",
    "        # Training parameters\n",
    "        tb.add_scalar('Learning rate (scheduler)', optimizer.param_groups[0][\"lr\"], epoch+1)\n",
    "        # NN Layers parameters\n",
    "        tb.add_histogram('conv1.bias', net.conv1.bias, epoch+1)\n",
    "        tb.add_histogram('conv1.weight', net.conv1.weight, epoch+1)\n",
    "        tb.add_histogram('conv1.weight.grad',net.conv1.weight.grad,epoch+1)\n",
    "        tb.add_histogram('bn1.bias', net.bn1.bias, epoch+1)\n",
    "        tb.add_histogram('bn1.weight', net.bn1.weight, epoch+1)\n",
    "        tb.add_histogram('bn1.weight.grad',net.bn1.weight.grad,epoch+1)      \n",
    "        tb.add_histogram('conv2.bias', net.conv2.bias, epoch+1)\n",
    "        tb.add_histogram('conv2.weight', net.conv2.weight, epoch+1)\n",
    "        tb.add_histogram('conv2.weight.grad',net.conv2.weight.grad,epoch+1)\n",
    "        tb.add_histogram('bn2.bias', net.bn2.bias, epoch+1)\n",
    "        tb.add_histogram('bn2.weight', net.bn2.weight, epoch+1)\n",
    "        tb.add_histogram('bn2.weight.grad',net.bn2.weight.grad,epoch+1)  \n",
    "        tb.add_histogram('fc1.bias', net.fc1.bias, epoch+1)\n",
    "        tb.add_histogram('fc1.weight', net.fc1.weight, epoch+1)\n",
    "        tb.add_histogram('fc1.weight.grad',net.fc1.weight.grad,epoch+1)\n",
    "        tb.add_histogram('fc2.bias', net.fc2.bias, epoch+1)\n",
    "        tb.add_histogram('fc2.weight', net.fc2.weight, epoch+1)\n",
    "        tb.add_histogram('fc2.weight.grad',net.fc2.weight.grad,epoch+1)\n",
    "        tb.add_histogram('fc3.bias', net.fc3.bias, epoch+1)\n",
    "        tb.add_histogram('fc3.weight', net.fc3.weight, epoch+1)\n",
    "        tb.add_histogram('fc3.weight.grad',net.fc3.weight.grad,epoch+1)\n",
    "        \n",
    "        \n",
    "        # Save best models\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_model_epoch = epoch\n",
    "            comment = f' trg_dataset{trg_dataset_ref} batch_size={batch_size} learning_rate={learning_rate} scheduler_step_size={scheduler_step_size} scheduler_gamma={scheduler_gamma} weight_decay={weight_decay} epoch_number={best_model_epoch} accuracy={val_accuracy}'\n",
    "            torch.save(net.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/model_' + date_now + '_' + time_now + '_' + comment + '.pth')\n",
    "            best_accuracy = val_accuracy          \n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    # At the end of the training, close the tensorboard SummaryWriter and save the model to the drive\n",
    "    tb.close()\n",
    "    print('Training Completed')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "learning_process_management.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
