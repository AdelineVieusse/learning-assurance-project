{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubIxRNwWjqQJ"
   },
   "source": [
    "# Model training stability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGC-enW3jqQJ"
   },
   "source": [
    "### Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VfIcKLF-FLk",
    "outputId": "ad2a220c-b668-46a6-d2bf-71aca05bb0dd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jqKo-I-jqQL"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM6EugCD-FLk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "import os\n",
    "import torchvision.models as tmodels\n",
    "from functools import partial\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code reused from https://colab.research.google.com/github/google-research/google-research/blob/master/representation_similarity/Demo.ipynb\n",
    "\n",
    "\"Similarity of Neural Network Representations Revisited\"\n",
    "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton\n",
    "https://arxiv.org/abs/1905.00414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMUef5Z2R2KV"
   },
   "outputs": [],
   "source": [
    "def gram_linear(x):\n",
    "  \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "  Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "\n",
    "  Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "  \"\"\"\n",
    "  return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "  \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "  Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "      possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "  Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "  \"\"\"\n",
    "  dot_products = x.dot(x.T)\n",
    "  sq_norms = np.diag(dot_products)\n",
    "  sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "  sq_median_distance = np.median(sq_distances)\n",
    "  return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "  \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "  This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "  induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "  Args:\n",
    "    gram: A num_examples x num_examples symmetric matrix.\n",
    "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "      estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "  Returns:\n",
    "    A symmetric matrix with centered columns and rows.\n",
    "  \"\"\"\n",
    "  if not np.allclose(gram, gram.T):\n",
    "    raise ValueError('Input must be a symmetric matrix.')\n",
    "  gram = gram.copy()\n",
    "\n",
    "  if unbiased:\n",
    "    # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "    # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "    # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "    # stable than the alternative from Song et al. (2007).\n",
    "    n = gram.shape[0]\n",
    "    np.fill_diagonal(gram, 0)\n",
    "    means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "    means -= np.sum(means) / (2 * (n - 1))\n",
    "    gram -= means[:, None]\n",
    "    gram -= means[None, :]\n",
    "    np.fill_diagonal(gram, 0)\n",
    "  else:\n",
    "    means = np.mean(gram, 0, dtype=np.float64)\n",
    "    means -= np.mean(means) / 2\n",
    "    gram -= means[:, None]\n",
    "    gram -= means[None, :]\n",
    "\n",
    "  return gram\n",
    "\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "  \"\"\"Compute CKA.\n",
    "\n",
    "  Args:\n",
    "    gram_x: A num_examples x num_examples Gram matrix.\n",
    "    gram_y: A num_examples x num_examples Gram matrix.\n",
    "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "  Returns:\n",
    "    The value of CKA between X and Y.\n",
    "  \"\"\"\n",
    "  gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "  gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "  # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "  # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "  scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "  normalization_x = np.linalg.norm(gram_x)\n",
    "  normalization_y = np.linalg.norm(gram_y)\n",
    "  return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(\n",
    "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
    "    n):\n",
    "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "  # This formula can be derived by manipulating the unbiased estimator from\n",
    "  # Song et al. (2007).\n",
    "  return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "  \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "  This is typically faster than computing the Gram matrix when there are fewer\n",
    "  features than examples.\n",
    "\n",
    "  Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "      biased. Note that this estimator may be negative.\n",
    "\n",
    "  Returns:\n",
    "    The value of CKA between X and Y.\n",
    "  \"\"\"\n",
    "  features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "  features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "  normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "  normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "  if debiased:\n",
    "    n = features_x.shape[0]\n",
    "    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "    squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "    squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "    dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "        squared_norm_x, squared_norm_y, n)\n",
    "    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "        squared_norm_x, squared_norm_x, n))\n",
    "    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "        squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "  return dot_product_similarity / (normalization_x * normalization_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey2mnm_CjqQN"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP7Ocm06-FLl"
   },
   "outputs": [],
   "source": [
    "norm_param_dataset_ref = \"AVG\"\n",
    "dataset_name = \"TEST_0_FINAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkW4imRc-FLl"
   },
   "outputs": [],
   "source": [
    "# Retrieve normalisation parameters \n",
    "\n",
    "norm_param_df = pd.read_csv('/content/drive/MyDrive/KASHIKO/DATASET/TRG_DATASET_NORM_PARAM.csv')\n",
    "\n",
    "meanR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanR\"].item()\n",
    "meanG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanG\"].item()\n",
    "meanB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanB\"].item()\n",
    "\n",
    "stdR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdR\"].item()\n",
    "stdG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdG\"].item()\n",
    "stdB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdB\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scrMU_PJ-FLm"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/KASHIKO/DATASET/' + dataset_name,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))\n",
    "    ])\n",
    ")\n",
    "_, short_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 10, 10])\n",
    "_, long_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 100, 100])\n",
    "_, extra_long_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 2000, 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-52PFYXjqQO"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhSz9Y9L-FLm"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(24*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1,24*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yBIrUwF-FLn"
   },
   "outputs": [],
   "source": [
    "net0 = Net()\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "net3 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ7SZaqP-FLn"
   },
   "outputs": [],
   "source": [
    "state_dict1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/testmodel_2021-05-28_05:59:14_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=14 accuracy=97.77070063694268.pth')\n",
    "state_dict2 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/testmodel_2021-05-28_07:34:20_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=11 accuracy=98.51380042462846.pth')\n",
    "state_dict3 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/testmodel_2021-05-28_08:37:05_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=14 accuracy=98.19532908704883.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyHrfS8h-FLn",
    "outputId": "210ea3c3-4c40-4c88-c328-42c5bb8ef5b5"
   },
   "outputs": [],
   "source": [
    "net1.load_state_dict(state_dict1)\n",
    "net2.load_state_dict(state_dict2)\n",
    "net3.load_state_dict(state_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3uJuyNMjqQP"
   },
   "source": [
    "### Generate activation file for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbvMOdns7mvt"
   },
   "outputs": [],
   "source": [
    "# Prepare loader for short_dataset\n",
    "loader = torch.utils.data.DataLoader(\n",
    "        short_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4gXMcDzArWf"
   },
   "outputs": [],
   "source": [
    "# For net0 (with short_dataset)\n",
    "activations0 = collections.defaultdict(list)\n",
    "def save_activation0(name, mod, inp, out0):\n",
    "    activations0[name].append(out0.cpu())\n",
    "    \n",
    "for name, m in net0.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net0.eval()\n",
    "        out0 = net0(images)\n",
    "\n",
    "activations0 = {name: torch.cat(outputs, 0) for name, outputs in activations0.items()}\n",
    "torch.save(activations0,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations0_short.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJPnG5r3jqQQ"
   },
   "outputs": [],
   "source": [
    "# For net1 (with short_dataset)\n",
    "activations1 = collections.defaultdict(list)\n",
    "def save_activation1(name, mod, inp, out1):\n",
    "    activations1[name].append(out1.cpu())\n",
    "    \n",
    "for name, m in net1.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net1.eval()\n",
    "        out1 = net1(images)\n",
    "\n",
    "activations1 = {name: torch.cat(outputs, 0) for name, outputs in activations1.items()}\n",
    "torch.save(activations1,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations1_short.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFTs-p_ajqQQ"
   },
   "outputs": [],
   "source": [
    "# For net2 (with short_dataset)\n",
    "activations2 = collections.defaultdict(list)\n",
    "def save_activation2(name, mod, inp, out2):\n",
    "    activations2[name].append(out2.cpu())\n",
    "    \n",
    "for name, m in net2.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net2.eval()\n",
    "        out2 = net2(images)\n",
    "\n",
    "activations2 = {name: torch.cat(outputs, 0) for name, outputs in activations2.items()}\n",
    "torch.save(activations2,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations2_short.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC0G2em-jqQR"
   },
   "outputs": [],
   "source": [
    "# For net3 (with short_dataset)\n",
    "activations3 = collections.defaultdict(list)\n",
    "def save_activation3(name, mod, inp, out3):\n",
    "    activations3[name].append(out3.cpu())\n",
    "    \n",
    "for name, m in net3.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net3.eval()\n",
    "        out3 = net3(images)\n",
    "\n",
    "activations3 = {name: torch.cat(outputs, 0) for name, outputs in activations3.items()}\n",
    "torch.save(activations3,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations3_short.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI5gDV3H7iJm"
   },
   "outputs": [],
   "source": [
    "# Prepare loader for long_dataset\n",
    "loader = torch.utils.data.DataLoader(\n",
    "        long_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0_Ify-s7iR9"
   },
   "outputs": [],
   "source": [
    "# Reload models\n",
    "net0 = Net()\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "net3 = Net()\n",
    "net1.load_state_dict(state_dict1)\n",
    "net2.load_state_dict(state_dict2)\n",
    "net3.load_state_dict(state_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_m3-5S0A-3I"
   },
   "outputs": [],
   "source": [
    "# For net0 (with long_dataset)\n",
    "activations0 = collections.defaultdict(list)\n",
    "def save_activation0(name, mod, inp, out0):\n",
    "    activations0[name].append(out0.cpu())\n",
    "    \n",
    "for name, m in net0.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation0, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net0.eval()\n",
    "        out0 = net0(images)\n",
    "\n",
    "activations0 = {name: torch.cat(outputs, 0) for name, outputs in activations0.items()}\n",
    "torch.save(activations0,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations0_long.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAexDU5f76U8"
   },
   "outputs": [],
   "source": [
    "# For net1 (with long_dataset)\n",
    "activations1 = collections.defaultdict(list)\n",
    "def save_activation1(name, mod, inp, out1):\n",
    "    activations1[name].append(out1.cpu())\n",
    "    \n",
    "for name, m in net1.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net1.eval()\n",
    "        out1 = net1(images)\n",
    "\n",
    "activations1 = {name: torch.cat(outputs, 0) for name, outputs in activations1.items()}\n",
    "torch.save(activations1,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations1_long.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OURxun2x76Vj"
   },
   "outputs": [],
   "source": [
    "# For net2 (with long_dataset)\n",
    "activations2 = collections.defaultdict(list)\n",
    "def save_activation2(name, mod, inp, out2):\n",
    "    activations2[name].append(out2.cpu())\n",
    "    \n",
    "for name, m in net2.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation2, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net2.eval()\n",
    "        out2 = net2(images)\n",
    "\n",
    "activations2 = {name: torch.cat(outputs, 0) for name, outputs in activations2.items()}\n",
    "torch.save(activations2,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations2_long.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHsH1cj276Vk"
   },
   "outputs": [],
   "source": [
    "# For net3 (with long_dataset)\n",
    "activations3 = collections.defaultdict(list)\n",
    "def save_activation3(name, mod, inp, out3):\n",
    "    activations3[name].append(out3.cpu())\n",
    "    \n",
    "for name, m in net3.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation3, name))\n",
    "        \n",
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net3.eval()\n",
    "        out3 = net3(images)\n",
    "\n",
    "activations3 = {name: torch.cat(outputs, 0) for name, outputs in activations3.items()}\n",
    "torch.save(activations3,'/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations3_long.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n9kXw_6jqQS"
   },
   "source": [
    "### Perform CKA analysis on Conv and BatchNorm layers (using short_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcSkLyeP8Vaz"
   },
   "outputs": [],
   "source": [
    "# Retrieve stored activations\n",
    "ACT0 = collections.defaultdict(list)\n",
    "ACT0 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations0_short.pt')\n",
    "\n",
    "ACT1 = collections.defaultdict(list)\n",
    "ACT1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations1_short.pt')\n",
    "\n",
    "ACT2 = collections.defaultdict(list)\n",
    "ACT2 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations2_short.pt')\n",
    "\n",
    "ACT3 = collections.defaultdict(list)\n",
    "ACT3 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations3_short.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Afd6-2WACdto"
   },
   "outputs": [],
   "source": [
    "# Conv1\n",
    "net0_conv1_activations = ACT0['conv1']\n",
    "net1_conv1_activations = ACT1['conv1']\n",
    "net2_conv1_activations = ACT2['conv1']\n",
    "net3_conv1_activations = ACT3['conv1']\n",
    "\n",
    "avg_acts0 = np.mean(net0_conv1_activations.numpy(), axis=(1,2))\n",
    "avg_acts1 = np.mean(net1_conv1_activations.numpy(), axis=(1,2))\n",
    "avg_acts2 = np.mean(net2_conv1_activations.numpy(), axis=(1,2))\n",
    "avg_acts3 = np.mean(net3_conv1_activations.numpy(), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roA0QrkyBWjf",
    "outputId": "d35a00a5-c268-4e07-9fc5-2448f0bac8cd"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts0))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts0)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts0, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts0), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts0, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9O68trmScK0",
    "outputId": "0abf1e66-5096-4ee2-9ca1-ae4a15d1f0ef"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts2))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts2)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts2, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts2), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts2, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgbGcUbZUxqk",
    "outputId": "0205c3b7-c927-4c92-8937-278062be273f"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyKY-0fPU6mz",
    "outputId": "5975ac88-8861-42ad-c91b-3dc278908dc5"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts2), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts2, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts2, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts2), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts2, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmKFqLTojqQS"
   },
   "outputs": [],
   "source": [
    "# Conv2\n",
    "net0_conv2_activations = ACT0['conv2']\n",
    "net1_conv2_activations = ACT1['conv2']\n",
    "net2_conv2_activations = ACT2['conv2']\n",
    "net3_conv2_activations = ACT3['conv2']\n",
    "\n",
    "avg_acts0 = np.mean(net0_conv2_activations.numpy(), axis=(1,2))\n",
    "avg_acts1 = np.mean(net1_conv2_activations.numpy(), axis=(1,2))\n",
    "avg_acts2 = np.mean(net2_conv2_activations.numpy(), axis=(1,2))\n",
    "avg_acts3 = np.mean(net3_conv2_activations.numpy(), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mUrT3CLDA9e",
    "outputId": "25944155-4fc3-4617-adb9-a02963593645"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts0))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts0)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts0, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts0), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts0, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAxMdFYaI4l4",
    "outputId": "d0f712a9-4020-45c9-8170-16750e88b297"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts2))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts2)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts2, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts2), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts2, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFNwOPSOI4l7",
    "outputId": "2f9d710b-78cd-44ec-909d-4b8f23645607"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iRymUBWI4l9",
    "outputId": "f01e8ef0-eeb2-4f9b-970d-1e1bad037710"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts2), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts2, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts2, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts2), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts2, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZzwORZL_WEB"
   },
   "outputs": [],
   "source": [
    "# BN1\n",
    "net0_bn1_activations = ACT0['bn1']\n",
    "net1_bn1_activations = ACT1['bn1']\n",
    "net2_bn1_activations = ACT2['bn1']\n",
    "net3_bn1_activations = ACT3['bn1']\n",
    "\n",
    "avg_acts0 = np.mean(net0_bn1_activations.numpy(), axis=(1,2))\n",
    "avg_acts1 = np.mean(net1_bn1_activations.numpy(), axis=(1,2))\n",
    "avg_acts2 = np.mean(net2_bn1_activations.numpy(), axis=(1,2))\n",
    "avg_acts3 = np.mean(net3_bn1_activations.numpy(), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzTtrl8MDUiO",
    "outputId": "9f306142-54af-4cda-8f64-472e06987b2a"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts0))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts0)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts0, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts0), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts0, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jd_dNq_NJlHv",
    "outputId": "ed93c084-ac39-43d6-94b0-beb7d272da44"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts2))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts2)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts2, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts2), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts2, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJw0V_dnJlHy",
    "outputId": "0af23aad-86bf-4772-a13e-1bbf1b971b8e"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJYP-xKRJlH0",
    "outputId": "d3d4dc8f-bb98-497e-e7fc-fe274c27cf30"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts2), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts2, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts2, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts2), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts2, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd_FHwv-_WNE"
   },
   "outputs": [],
   "source": [
    "# BN2\n",
    "net0_bn2_activations = ACT0['bn2']\n",
    "net1_bn2_activations = ACT1['bn2']\n",
    "net2_bn2_activations = ACT2['bn2']\n",
    "net3_bn2_activations = ACT3['bn2']\n",
    "\n",
    "avg_acts0 = np.mean(net0_bn2_activations.numpy(), axis=(1,2))\n",
    "avg_acts1 = np.mean(net1_bn2_activations.numpy(), axis=(1,2))\n",
    "avg_acts2 = np.mean(net2_bn2_activations.numpy(), axis=(1,2))\n",
    "avg_acts3 = np.mean(net3_bn2_activations.numpy(), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1zu4ncFDitO",
    "outputId": "517ae576-fddc-4365-afbc-cf52703d8533"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts0))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts0)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts0, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts0), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts0, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HuWa-mnwM_-Y",
    "outputId": "ddd50962-ee9c-4733-a0fb-31dc80501602"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts2))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts2)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts2, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts2), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts2, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX9FrqXmM_-a",
    "outputId": "9397590b-44e1-4d9e-80bd-4618b7e49806"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts1), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts1, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts1, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts1), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts1, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QBzj5FBM0Pa",
    "outputId": "13f5acde-edf3-4d3a-a121-33beae04f109"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(avg_acts2), gram_linear(avg_acts3))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(avg_acts2, avg_acts3)\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(avg_acts2, 0.3), gram_rbf(avg_acts3, 0.3))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(avg_acts2), gram_linear(avg_acts3), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(avg_acts2, avg_acts3, debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQyh5QhU6rbm"
   },
   "source": [
    "### CKA analysis for Fully Connected Layers (using long_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz5hq1066rgo"
   },
   "outputs": [],
   "source": [
    "# Retrieve stored activations\n",
    "ACT0 = collections.defaultdict(list)\n",
    "ACT0 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations0_long.pt')\n",
    "\n",
    "ACT1 = collections.defaultdict(list)\n",
    "ACT1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations1_long.pt')\n",
    "\n",
    "ACT2 = collections.defaultdict(list)\n",
    "ACT2 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations2_long.pt')\n",
    "\n",
    "ACT3 = collections.defaultdict(list)\n",
    "ACT3 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/stability_analysis_activations3_long.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FrJASX5_WPv"
   },
   "outputs": [],
   "source": [
    "net0_fc1_activations = ACT0['fc1']\n",
    "net1_fc1_activations = ACT1['fc1']\n",
    "net2_fc1_activations = ACT2['fc1']\n",
    "net3_fc1_activations = ACT3['fc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_C_I6UlD1Gu",
    "outputId": "9a932dfa-846c-4eed-f429-47f9a190849e"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net0_fc1_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc1_activations.numpy(), net0_fc1_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc1_activations.numpy(), 0.5), gram_rbf(net0_fc1_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net0_fc1_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc1_activations.numpy(), net0_fc1_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4Jge-_RY0gD",
    "outputId": "0a8f9302-4290-4f0e-a8f8-4d4c74ee242e"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net2_fc1_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc1_activations.numpy(), net2_fc1_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc1_activations.numpy(), 0.5), gram_rbf(net2_fc1_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net2_fc1_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc1_activations.numpy(), net2_fc1_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zepBTzxPdKjy",
    "outputId": "af432c6c-08c5-4d64-8ae3-4b662a4d9117"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net3_fc1_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc1_activations.numpy(), net3_fc1_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc1_activations.numpy(), 0.5), gram_rbf(net3_fc1_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc1_activations.numpy()), gram_linear(net3_fc1_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc1_activations.numpy(), net3_fc1_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guZ7DF2PN1ah",
    "outputId": "1dd005c2-23cb-49cb-d876-4da9e860ab4b"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net2_fc1_activations.numpy()), gram_linear(net3_fc1_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net2_fc1_activations.numpy(), net3_fc1_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net2_fc1_activations.numpy(), 0.5), gram_rbf(net3_fc1_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net2_fc1_activations.numpy()), gram_linear(net3_fc1_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net2_fc1_activations.numpy(), net3_fc1_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ao-1KYSdK2L"
   },
   "outputs": [],
   "source": [
    "net0_fc2_activations = ACT0['fc2']\n",
    "net1_fc2_activations = ACT1['fc2']\n",
    "net2_fc2_activations = ACT2['fc2']\n",
    "net3_fc2_activations = ACT3['fc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bmGCXXsEHJT",
    "outputId": "d437304e-07d9-47b7-c4da-29268a01db7e"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net0_fc2_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc2_activations.numpy(), net0_fc2_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc2_activations.numpy(), 0.5), gram_rbf(net0_fc2_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net0_fc2_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc2_activations.numpy(), net0_fc2_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djKD3boHOBZ8",
    "outputId": "4ccf9a99-f31c-4299-f3f5-cace0bfd5651"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net2_fc2_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc2_activations.numpy(), net2_fc2_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc2_activations.numpy(), 0.5), gram_rbf(net2_fc2_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net2_fc2_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc2_activations.numpy(), net2_fc2_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Efb3nI2EOBZ_",
    "outputId": "7ad936c5-1565-4741-d383-9d85a835a1f3"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net3_fc2_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc2_activations.numpy(), net3_fc2_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc2_activations.numpy(), 0.5), gram_rbf(net3_fc2_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc2_activations.numpy()), gram_linear(net3_fc2_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc2_activations.numpy(), net3_fc2_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWg_zPYSOBaA",
    "outputId": "91046ca6-3527-46c3-b2e2-9525e0b85f86"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net2_fc2_activations.numpy()), gram_linear(net3_fc2_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net2_fc2_activations.numpy(), net3_fc2_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net2_fc2_activations.numpy(), 0.5), gram_rbf(net3_fc2_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net2_fc2_activations.numpy()), gram_linear(net3_fc2_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net2_fc2_activations.numpy(), net3_fc2_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zerFIhOWCUdL"
   },
   "outputs": [],
   "source": [
    "net0_fc3_activations = ACT0['fc3']\n",
    "net1_fc3_activations = ACT1['fc3']\n",
    "net2_fc3_activations = ACT2['fc3']\n",
    "net3_fc3_activations = ACT3['fc3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuZt75ywETsm",
    "outputId": "e95a5208-f714-48eb-9c3e-a69707567a93"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net0_fc3_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc3_activations.numpy(), net0_fc3_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc3_activations.numpy(), 0.5), gram_rbf(net0_fc3_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net0_fc3_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc3_activations.numpy(), net0_fc3_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmUyi-tEPJwn",
    "outputId": "5441a7bc-8347-4c96-d421-eb80f48c2dd9"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net2_fc3_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc3_activations.numpy(), net2_fc3_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc3_activations.numpy(), 0.5), gram_rbf(net2_fc3_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net2_fc3_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc3_activations.numpy(), net2_fc3_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZGzHljIPJwy",
    "outputId": "083fb9ed-9873-4ad2-a34b-d1446f98eddd"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net3_fc3_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net1_fc3_activations.numpy(), net3_fc3_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net1_fc3_activations.numpy(), 0.5), gram_rbf(net3_fc3_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net1_fc3_activations.numpy()), gram_linear(net3_fc3_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net1_fc3_activations.numpy(), net3_fc3_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irFQFbefPJw0",
    "outputId": "f22d2cce-5bea-4856-f59a-dbbd5c729c62"
   },
   "outputs": [],
   "source": [
    "cka_from_examples = cka(gram_linear(net2_fc3_activations.numpy()), gram_linear(net3_fc3_activations.numpy()))\n",
    "print('Linear CKA from Examples: {:.5f}'.format(cka_from_examples))\n",
    "\n",
    "cka_from_features = feature_space_linear_cka(net2_fc3_activations.numpy(), net3_fc3_activations.numpy())\n",
    "print('Linear CKA from Features: {:.5f}'.format(cka_from_features))\n",
    "\n",
    "rbf_cka = cka(gram_rbf(net2_fc3_activations.numpy(), 0.5), gram_rbf(net3_fc3_activations.numpy(), 0.5))\n",
    "print('RBF CKA: {:.5f}'.format(rbf_cka))\n",
    "\n",
    "cka_from_examples_debiased = cka(gram_linear(net2_fc3_activations.numpy()), gram_linear(net3_fc3_activations.numpy()), debiased=True)\n",
    "print('Linear CKA from Examples (Debiased): {:.5f}'.format(cka_from_examples_debiased))\n",
    "\n",
    "cka_from_features_debiased = feature_space_linear_cka(net2_fc3_activations.numpy(), net3_fc3_activations.numpy(), debiased=True)\n",
    "print('Linear CKA from Features (Debiased): {:.5f}'.format(cka_from_features_debiased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRkbPYscDVCy"
   },
   "source": [
    "### Perform Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0iHcERtD0qz"
   },
   "outputs": [],
   "source": [
    "# Prepare loader for long_dataset\n",
    "loader = torch.utils.data.DataLoader(\n",
    "        extra_long_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwNrZ3TaC1_s",
    "outputId": "105cf767-cc68-4e57-c8be-c8222e8db726"
   },
   "outputs": [],
   "source": [
    "# Reload models\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "net3 = Net()\n",
    "net1.load_state_dict(state_dict1)\n",
    "net2.load_state_dict(state_dict2)\n",
    "net3.load_state_dict(state_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFcp78IiDVCy",
    "outputId": "e06df6e7-2eba-4dcf-f8fe-7defde85b7d6"
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "same12 = 0\n",
    "same13 = 0\n",
    "same23 = 0\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "correct3 = 0\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        net1.eval()\n",
    "        out1 = net1(images)\n",
    "        net2.eval()\n",
    "        out2 = net2(images)\n",
    "        net3.eval()\n",
    "        out3 = net3(images)\n",
    "\n",
    "        _, predicted1 = torch.max(out1.data, 1)\n",
    "        _, predicted2 = torch.max(out2.data, 1)\n",
    "        _, predicted3 = torch.max(out3.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct1 += (predicted1 == labels).sum().item()\n",
    "        correct2 += (predicted2 == labels).sum().item()\n",
    "        correct3 += (predicted3 == labels).sum().item()\n",
    "\n",
    "        same12 += (predicted1 == predicted2).sum().item()\n",
    "        same13 += (predicted1 == predicted3).sum().item()\n",
    "        same23 += (predicted3 == predicted2).sum().item()\n",
    "        \n",
    "test_accuracy1 = 100 * correct1 / total\n",
    "test_accuracy2 = 100 * correct2 / total\n",
    "test_accuracy3 = 100 * correct3 / total\n",
    "\n",
    "same12_accuracy = 100 * same12 / total\n",
    "same13_accuracy = 100 * same13 / total\n",
    "same23_accuracy = 100 * same23 / total\n",
    "\n",
    "\n",
    "print('Accuracy Net1: {:.5f}'.format(test_accuracy1))\n",
    "print('Accuracy Net2: {:.5f}'.format(test_accuracy2))\n",
    "print('Accuracy Net3: {:.5f}'.format(test_accuracy3))\n",
    "\n",
    "print('Consistency Net1 vs Net2: {:.5f}'.format(same12_accuracy))\n",
    "print('Consistency Net1 vs Net3: {:.5f}'.format(same13_accuracy))\n",
    "print('Consistency Net2 vs Net3: {:.5f}'.format(same23_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2XDFLMMHT7_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "stability_analysis_cca_cka_results.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
