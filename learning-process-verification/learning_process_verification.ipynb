{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiOpYzYDMrTO"
   },
   "source": [
    "# Learning Process Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BsHuiQyMrTO"
   },
   "source": [
    "### Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JapbtNfFez8X",
    "outputId": "091fb05b-2cd0-4a63-821c-35d199dd7079"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUs3cxUnMrTQ"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXVMI9xUez8Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJki2Y_vMrTR"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCw3LmettHGm"
   },
   "outputs": [],
   "source": [
    "# Retrieve normalisation parameters \n",
    "\n",
    "norm_param_df = pd.read_csv('/content/drive/MyDrive/KASHIKO/DATASET/TRG_DATASET_NORM_PARAM.csv')\n",
    "\n",
    "meanR = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"meanR\"].item()\n",
    "meanG = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"meanG\"].item()\n",
    "meanB = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"meanB\"].item()\n",
    "\n",
    "stdR = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"stdR\"].item()\n",
    "stdG = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"stdG\"].item()\n",
    "stdB = norm_param_df.loc[norm_param_df[\"Dataset\"] == \"AVG\", \"stdB\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWevFmdjez8a"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/KASHIKO/DATASET/TEST_0_FINAL/',\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))\n",
    "    ])\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sWyV09DMrTS"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwdU4ME7ez8a"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(24*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1,24*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9CdJ-chez8b",
    "outputId": "5a8e19d1-02ff-45bf-cff0-f578bcd6d073"
   },
   "outputs": [],
   "source": [
    "net1 = Net()\n",
    "state_dict1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-29_12:16:11_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=11 accuracy=97.6.pth')\n",
    "net1.load_state_dict(state_dict1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SKJ8Hm2MrTT",
    "outputId": "2220588a-2815-4b08-b3ae-c8f8c41fdcde"
   },
   "outputs": [],
   "source": [
    "net2 = Net()\n",
    "state_dict2 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-29_18:41:53_ trg_dataset2 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=16 accuracy=98.1.pth')\n",
    "net2.load_state_dict(state_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUM0lWgRMrTU",
    "outputId": "6c0482f8-01f2-40e3-a8b8-b91496e98c3f"
   },
   "outputs": [],
   "source": [
    "net3 = Net()\n",
    "state_dict3 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-30_08:24:13_ trg_dataset3 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=19 accuracy=98.2.pth')\n",
    "net3.load_state_dict(state_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u578yo9GMrTU"
   },
   "source": [
    "### Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jOQeSH5TTqD",
    "outputId": "4d51f4b8-545b-4e60-e79d-43527fe01037"
   },
   "outputs": [],
   "source": [
    "# Test model 1\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        net1.eval()\n",
    "        out = net1(images)\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "print(test_accuracy_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdRUlXEpTlBk",
    "outputId": "0551a166-86c4-4333-ee20-4a36b75227fd"
   },
   "outputs": [],
   "source": [
    "# Test model 2\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        net2.eval()\n",
    "        out = net2(images)\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "print(test_accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dnt2SdmTlJa",
    "outputId": "6c602a9c-9b4b-485a-a1ed-d784cdc28866"
   },
   "outputs": [],
   "source": [
    "# Test model 3\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        net3.eval()\n",
    "        out = net3(images)\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "print(test_accuracy_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haTRx3vpcez6"
   },
   "source": [
    "### Can accuracy be improved by implementing a voting system (with and without confidence threshold)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvChU5AFF1vh",
    "outputId": "be4bfe17-cb66-45a3-85aa-235468ff4d2a"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_test_param_column_list = ['TRUE','PREDICTED1','CORRECT1','SURE1','PREDICTED2','CORRECT2','SURE2','PREDICTED3','CORRECT3','SURE3','FINAL_PREDICTION']\n",
    "for threshold in [0,0.7,0.8,0.9,0.95,0.96,0.97,0.98]:\n",
    "    total_all1 = 0\n",
    "    correct_all1 = 0\n",
    "    total_sure1 = 0\n",
    "    correct_sure1 = 0\n",
    "    total_all2 = 0\n",
    "    correct_all2 = 0\n",
    "    total_sure2 = 0\n",
    "    correct_sure2 = 0\n",
    "    total_all3 = 0\n",
    "    correct_all3 = 0\n",
    "    total_sure3 = 0\n",
    "    correct_sure3 = 0\n",
    "    total_final = 0\n",
    "    correct_final = 0\n",
    "    model_test_param_df = pd.DataFrame(columns=model_test_param_column_list)\n",
    "\n",
    "    m = nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            net1.eval()\n",
    "            out1 = net1(images)        \n",
    "            net2.eval()\n",
    "            out2 = net2(images)        \n",
    "            net3.eval()\n",
    "            out3 = net3(images)\n",
    "\n",
    "            _, predicted1 = torch.max(out1.data, 1)\n",
    "            _, predicted2 = torch.max(out2.data, 1)\n",
    "            _, predicted3 = torch.max(out3.data, 1)\n",
    "          \n",
    "            predicted_soft1 = m(out1)\n",
    "            predicted_soft2 = m(out2)\n",
    "            predicted_soft3 = m(out3)\n",
    "            true = labels.item()\n",
    "\n",
    "            if np.amax(predicted_soft1.numpy()) > threshold:\n",
    "                total_sure1 += labels.size(0)\n",
    "                correct_sure1 += (predicted1 == labels).sum().item()\n",
    "            total_all1 += labels.size(0)\n",
    "            correct_all1 += (predicted1 == labels).sum().item()\n",
    "            prediction1 = predicted1.item()\n",
    "            if predicted1 == labels:\n",
    "                correct1 = 1\n",
    "            else:\n",
    "                correct1 = 0\n",
    "\n",
    "            if np.amax(predicted_soft1.numpy()) > threshold:\n",
    "                sure1 = 1\n",
    "            else:\n",
    "                sure1 = 0\n",
    "          \n",
    "            if np.amax(predicted_soft2.numpy()) > threshold:\n",
    "                total_sure2 += labels.size(0)\n",
    "                correct_sure2 += (predicted2 == labels).sum().item()\n",
    "            total_all2 += labels.size(0)\n",
    "            correct_all2 += (predicted2 == labels).sum().item()\n",
    "            prediction2 = predicted2.item()\n",
    "            if predicted2 == labels:\n",
    "                correct2 = 1\n",
    "            else:\n",
    "                correct2 = 0\n",
    "\n",
    "            if np.amax(predicted_soft2.numpy()) > threshold:\n",
    "                sure2 = 1\n",
    "            else:\n",
    "                sure2 = 0\n",
    "\n",
    "            if np.amax(predicted_soft3.numpy()) > threshold:\n",
    "                total_sure3 += labels.size(0)\n",
    "                correct_sure3 += (predicted3 == labels).sum().item()\n",
    "            total_all3 += labels.size(0)\n",
    "            correct_all3 += (predicted3 == labels).sum().item()\n",
    "            prediction3 = predicted3.item()\n",
    "            if predicted3 == labels:\n",
    "                correct3 = 1\n",
    "            else:\n",
    "                correct3 = 0\n",
    "\n",
    "            if np.amax(predicted_soft3.numpy()) > threshold:\n",
    "                sure3 = 1\n",
    "            else:\n",
    "                sure3 = 0\n",
    "\n",
    "\n",
    "            if sure1 + sure2 + sure3 == 3:\n",
    "                if prediction1 + prediction2 + prediction3 >=2:\n",
    "                    final_prediction = 1\n",
    "                else:\n",
    "                    final_prediction = 0\n",
    "            elif sure1 + sure2 + sure3 == 2:\n",
    "                if sure1 == 0:\n",
    "                    if prediction2 + prediction3 == 2:\n",
    "                        final_prediction = 1\n",
    "                    elif prediction2 + prediction3 == 0:\n",
    "                        final_prediction = 0\n",
    "                    else:\n",
    "                        final_prediction = -1\n",
    "                if sure2 == 0:\n",
    "                    if prediction1 + prediction3 ==2:\n",
    "                        final_prediction = 1\n",
    "                    elif prediction1 + prediction3 ==0:\n",
    "                        final_prediction = 0\n",
    "                    else:\n",
    "                        final_prediction = -1\n",
    "                if sure3 == 0:\n",
    "                    if prediction2 + prediction1 ==2:\n",
    "                        final_prediction = 1\n",
    "                elif prediction2 + prediction1 ==0:\n",
    "                    final_prediction = 0\n",
    "                else:\n",
    "                    final_prediction = -1\n",
    "            else:\n",
    "                final_prediction = -1\n",
    "          \n",
    "            if final_prediction != -1:\n",
    "                total_final += labels.size(0)\n",
    "                correct_final += (final_prediction == labels).sum().item()\n",
    "\n",
    "\n",
    "            temp_df = pd.DataFrame([[true, prediction1, correct1, sure1,\n",
    "                                  prediction2, correct2, sure2,\n",
    "                                  prediction3, correct3, sure3,\n",
    "                                  final_prediction]], columns=model_test_param_column_list)\n",
    "            model_test_param_df = model_test_param_df.append(temp_df)\n",
    "\n",
    "\n",
    "    test_accuracy_all1 = 100 * correct_all1 / total_all1\n",
    "    test_accuracy_sure1 = 100 * correct_sure1 / total_sure1\n",
    "    test_accuracy_all2 = 100 * correct_all2 / total_all2\n",
    "    test_accuracy_sure2 = 100 * correct_sure2 / total_sure2\n",
    "    test_accuracy_all3 = 100 * correct_all3 / total_all3\n",
    "    test_accuracy_sure3 = 100 * correct_sure3 / total_sure3\n",
    "    test_accuracy_final = 100 * correct_final / total_final\n",
    "    print(\"Threshold:\", threshold)\n",
    "    print(\"Model1:\", test_accuracy_all1, test_accuracy_sure1, 100 * total_sure1/total_all1)\n",
    "    print(\"Model2:\", test_accuracy_all2, test_accuracy_sure2, 100 * total_sure2/total_all2)\n",
    "    print(\"Model3:\", test_accuracy_all3, test_accuracy_sure3, 100 * total_sure3/total_all3)\n",
    "    print(\"Final: \", test_accuracy_final, '                 ', 100 * total_final/total_all3)\n",
    "\n",
    "model_test_param_df.to_csv('/content/drive/MyDrive/KASHIKO/test_model123.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "learning_process_verification_1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
