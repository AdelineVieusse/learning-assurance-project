{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s82Bc2wy1Odn"
   },
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56zSm6obhkl"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--M8oALDbdNG",
    "outputId": "3125671a-295f-479f-bed5-c86c9849a807"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTXKSXd6bkId"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9whJki3wBgj",
    "outputId": "de955bac-d422-4ed4-94de-b3753904d2a2"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade nni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z74FPuQj1Odp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.prune import identity\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "import os\n",
    "import torchvision.models as tmodels\n",
    "from functools import partial\n",
    "import collections\n",
    "import nni\n",
    "from nni.algorithms.compression.pytorch.quantization import NaiveQuantizer\n",
    "from nni.algorithms.compression.pytorch.pruning import LevelPruner\n",
    "import time\n",
    "from nni.algorithms.compression.pytorch.pruning import AGPPruner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMXQOFgEbnvl"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOhCaOVQY-JY"
   },
   "outputs": [],
   "source": [
    "norm_param_dataset_ref = 3\n",
    "dataset_name = \"TRG_3_FINAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7FZ8TMWY-v-"
   },
   "outputs": [],
   "source": [
    "# Retrieve normalisation parameters \n",
    "\n",
    "norm_param_df = pd.read_csv('/content/drive/MyDrive/KASHIKO/DATASET/TRG_DATASET_NORM_PARAM.csv')\n",
    "\n",
    "meanR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanR\"].item()\n",
    "meanG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanG\"].item()\n",
    "meanB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"meanB\"].item()\n",
    "\n",
    "stdR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdR\"].item()\n",
    "stdG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdG\"].item()\n",
    "stdB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(norm_param_dataset_ref), \"stdB\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LwDP6__1Odq"
   },
   "outputs": [],
   "source": [
    "# Prepare normalized dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/KASHIKO/DATASET/' + dataset_name,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 1000, 1000])\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsRW1URNbpr9"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCCbmREt1Odr"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(24*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1,24*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvvo20aL1Odr",
    "outputId": "3c327737-94a3-43d0-cc08-f204daa41996"
   },
   "outputs": [],
   "source": [
    "net1 = Net()\n",
    "state_dict1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-29_12:16:11_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=11 accuracy=97.6.pth')\n",
    "net1.load_state_dict(state_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyRu2cwj1Odr",
    "outputId": "bbccf51b-59f5-4ae2-e131-cb3831152c0a"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "state_dict2 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-29_18:41:53_ trg_dataset2 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=16 accuracy=98.1.pth')\n",
    "net.load_state_dict(state_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHoPXQl31Ods",
    "outputId": "020c73d3-2b75-4a64-945f-ec57c27a9227"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "state_dict3 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-30_08:24:13_ trg_dataset3 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=19 accuracy=98.2.pth')\n",
    "net.load_state_dict(state_dict3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf23Zb-8hc50"
   },
   "outputs": [],
   "source": [
    "# Set temperature scaling coefficients (values computed previously)\n",
    "temp_factor_net1 = 3.661\n",
    "temp_factor_net2 = 4.289\n",
    "temp_factor_net3 = 3.913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoMRsfVS9HLU"
   },
   "source": [
    "### Convert models to lower version of Pytorch for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSCsdNcq9LBK"
   },
   "outputs": [],
   "source": [
    "torch.save(net1.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/best_model1.pth', _use_new_zipfile_serialization=False)\n",
    "torch.save(net2.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/best_model2.pth', _use_new_zipfile_serialization=False)\n",
    "torch.save(net3.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/best_model3.pth', _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyJSlqFyaH-"
   },
   "source": [
    "### Test perfomance (accuracy and speed) of pre-pruning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4eXRSTWyaIA",
    "outputId": "d0bc8981-5d6c-4d0f-e31f-d6fc2bae0a3b"
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "# Set metrics to 0\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "total_sure = 0\n",
    "correct_sure = 0\n",
    "total_sure_temp = 0\n",
    "correct_sure_temp = 0\n",
    "# Define softmax function\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "# Perform a fowrad pass of the dataset through the unpruned model\n",
    "# and compute performance and time to complete processing\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        net.eval()\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        predicted_soft = m(out)\n",
    "        predicted_soft_temp = m(out/temp_factor_net2)\n",
    "        if np.amax(predicted_soft.numpy()) > 0.97:\n",
    "            total_sure += labels.size(0)\n",
    "            correct_sure += (predicted == labels).sum().item()\n",
    "        if np.amax(predicted_soft_temp.numpy()) > 0.73:\n",
    "            total_sure_temp += labels.size(0)\n",
    "            correct_sure_temp += (predicted == labels).sum().item()\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "        \n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "test_accuracy_sure = 100 * correct_sure / total_sure\n",
    "test_accuracy_sure_temp = 100 * correct_sure_temp / total_sure_temp\n",
    "\n",
    "print(test_accuracy_all)\n",
    "print(test_accuracy_sure)\n",
    "print(100 * total_sure/total_all)\n",
    "print(test_accuracy_sure_temp)\n",
    "print(100 * total_sure_temp/total_all)\n",
    "\n",
    "end_time = time.clock()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b56sZR5qyTg5"
   },
   "source": [
    "### Apply LevelPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pd5JYX_yTjg",
    "outputId": "5bf6ed98-ce60-4a57-f79c-fabb61baf676"
   },
   "outputs": [],
   "source": [
    "# Define pruning parameters\n",
    "config_list = [{ 'sparsity': 0.3, 'op_types': ['default'] }]\n",
    "\n",
    "# Apply pruning to model\n",
    "pruner = LevelPruner(net, config_list)\n",
    "pruner.compress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyJSlqFyaH-"
   },
   "source": [
    "### Test perfomance (accuracy and speed) of post-pruning pre-retraining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4eXRSTWyaIA",
    "outputId": "d0bc8981-5d6c-4d0f-e31f-d6fc2bae0a3b"
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "# Set metrics to 0\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "total_sure = 0\n",
    "correct_sure = 0\n",
    "total_sure_temp = 0\n",
    "correct_sure_temp = 0\n",
    "# Define softmax function\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "# Perform a fowrad pass of the dataset through the pruned (but not retrained) model\n",
    "# and compute performance and time to complete processing\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        net.eval()\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        predicted_soft = m(out)\n",
    "        predicted_soft_temp = m(out/temp_factor_net2)\n",
    "        if np.amax(predicted_soft.numpy()) > 0.97:\n",
    "            total_sure += labels.size(0)\n",
    "            correct_sure += (predicted == labels).sum().item()\n",
    "        if np.amax(predicted_soft_temp.numpy()) > 0.73:\n",
    "            total_sure_temp += labels.size(0)\n",
    "            correct_sure_temp += (predicted == labels).sum().item()\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "        \n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "test_accuracy_sure = 100 * correct_sure / total_sure\n",
    "test_accuracy_sure_temp = 100 * correct_sure_temp / total_sure_temp\n",
    "\n",
    "print(test_accuracy_all)\n",
    "print(test_accuracy_sure)\n",
    "print(100 * total_sure/total_all)\n",
    "print(test_accuracy_sure_temp)\n",
    "print(100 * total_sure_temp/total_all)\n",
    "\n",
    "end_time = time.clock()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining pruned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OTDSYQDyTmY"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8sDYnoOyTox",
    "outputId": "d5effe55-4e11-4d35-8ff1-bce8eb9bea35"
   },
   "outputs": [],
   "source": [
    "# Load pruned model\n",
    "net1_pruned = LevelPruner(net1, config_list)#, optimizer, pruning_algorithm='level')\n",
    "net1_pruned.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wp86jyi32ocp"
   },
   "outputs": [],
   "source": [
    "# Set training hyperparameters\n",
    "parameters = dict(learning_rate = [0.001],\n",
    "                  batch_size = [100],\n",
    "                  weight_decay = [0],\n",
    "                  epoch_number = [5],\n",
    "                  scheduler_step_size = [5],\n",
    "                  scheduler_gamma = [1]   )\n",
    "param_values = [v for v in parameters.values()]\n",
    "trg_dataset_ref = 3\n",
    "valloader_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEe9ubXV5Rvb",
    "outputId": "13a07bc0-75c9-4526-adab-5562ab18cd0f"
   },
   "outputs": [],
   "source": [
    "# Perform model training\n",
    "for learning_rate, batch_size, weight_decay, epoch_number, scheduler_step_size, scheduler_gamma in product(*param_values): \n",
    "\n",
    "    # Define Optimizer and scheduler\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize tensorboard SummaryWriter file/directory\n",
    "    date_now = str(date.today())\n",
    "    time_now = datetime.now().strftime(\"%H:%M:%S\")    \n",
    "    log_dir_root = os.path.join('/content/drive/My Drive/KASHIKO/RUNS', date_now + '_' + time_now + '_')\n",
    "    comment = f' trg_dataset{trg_dataset_ref} batch_size={batch_size} learning_rate={learning_rate} scheduler_step_size={scheduler_step_size} scheduler_gamma={scheduler_gamma} weight_decay={weight_decay} epoch_number={epoch_number}'\n",
    "    log_dir = log_dir_root + comment\n",
    "    tb = SummaryWriter(log_dir)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epoch_number):  # loop over the dataset multiple times\n",
    "\n",
    "        net.train()\n",
    "        net.requires_grad = True\n",
    "        trg_running_loss = 0.0\n",
    "        trg_epoch_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            trg_running_loss += loss.item()\n",
    "            trg_epoch_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, trg_running_loss / 10))\n",
    "                trg_running_loss = 0.0\n",
    "\n",
    "        # At the end of each epoch, check the performance of the network using the validation dataset\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        TP = 0.0\n",
    "        TN = 0.0\n",
    "        FP = 0.0\n",
    "        FN = 0.0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                net.eval()\n",
    "                images, labels = data\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                TP += (predicted == labels & labels == 0).sum().item()\n",
    "                TN += (predicted == labels & labels == 1).sum().item()\n",
    "                FP += (predicted != labels & predicted == 0).sum().item()\n",
    "                FN += (predicted != labels & predicted == 1).sum().item()\n",
    "                val_loss += criterion(outputs, labels)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_sensitivity_TPR = TP/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        val_specificity_TNR = TN/(TN+FP) if (TN+FP)!=0.0 else -1.0\n",
    "        val_FPR = FP/(TN+FP) if (TN+FP)!=0.0 else -1.0\n",
    "        val_FNR = FN/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        val_precision = TP/(TP+FP) if (TP+FP)!=0.0 else -1.0\n",
    "        val_recall = TP/(FN+TP) if (FN+TP)!=0.0 else -1.0\n",
    "        inv_val_recall = 1/val_recall if val_recall!=0.0 else -1.0\n",
    "        inv_val_precision = 1/val_precision if val_precision!=0.0 else -1.0\n",
    "        val_F1_score = 2/(inv_val_precision + inv_val_recall) if (inv_val_precision + inv_val_recall)!=0.0 else -1.0\n",
    "        print(f'Accuracy of the network on the 1000 test images:{val_accuracy}')\n",
    "\n",
    "        # Store metrics and other parameters in tensorboard SummaryWriter\n",
    "        # Metrics\n",
    "        tb.add_scalar('Training Loss', trg_epoch_loss/(int((len(dataset)-valloader_size)/batch_size) * batch_size), epoch+1)\n",
    "        tb.add_scalar('Validation Loss', val_loss/valloader_size, epoch+1)\n",
    "        tb.add_scalar('Accuracy', val_accuracy, epoch+1)\n",
    "        tb.add_scalar('Sensitivity TPR', val_sensitivity_TPR, epoch+1)\n",
    "        tb.add_scalar('Specificity TNR', val_specificity_TNR, epoch+1)        \n",
    "        tb.add_scalar('FPR', val_FPR, epoch+1)\n",
    "        tb.add_scalar('FNR', val_FNR, epoch+1)        \n",
    "        tb.add_scalar('Precision', val_precision, epoch+1)\n",
    "        tb.add_scalar('Recall', val_recall, epoch+1)        \n",
    "        tb.add_scalar('F1 Score', val_F1_score, epoch+1)\n",
    "        # DEBUG\n",
    "        tb.add_scalar('False Positive', FP, epoch+1)        \n",
    "        tb.add_scalar('False Negative', FN, epoch+1)\n",
    "        tb.add_scalar('True Positive', TP, epoch+1)        \n",
    "        tb.add_scalar('True Negative', TN, epoch+1)\n",
    "        # Training parameters\n",
    "        tb.add_scalar('Learning rate (scheduler)', optimizer.param_groups[0][\"lr\"], epoch+1)\n",
    "        # NN Layers parameters\n",
    "        #tb.add_histogram('conv1.bias', net.conv1.bias, epoch+1)\n",
    "        #tb.add_histogram('conv1.weight', net.conv1.weight, epoch+1)\n",
    "        #tb.add_histogram('conv1.weight.grad',net.conv1.weight.grad,epoch+1)\n",
    "        #tb.add_histogram('bn1.bias', net.bn1.bias, epoch+1)\n",
    "        #tb.add_histogram('bn1.weight', net.bn1.weight, epoch+1)\n",
    "        #tb.add_histogram('bn1.weight.grad',net.bn1.weight.grad,epoch+1)      \n",
    "        #tb.add_histogram('conv2.bias', net.conv2.bias, epoch+1)\n",
    "        #tb.add_histogram('conv2.weight', net.conv2.weight, epoch+1)\n",
    "        #tb.add_histogram('conv2.weight.grad',net.conv2.weight.grad,epoch+1)\n",
    "        #tb.add_histogram('bn2.bias', net.bn2.bias, epoch+1)\n",
    "        #tb.add_histogram('bn2.weight', net.bn2.weight, epoch+1)\n",
    "        #tb.add_histogram('bn2.weight.grad',net.bn2.weight.grad,epoch+1)  \n",
    "        #tb.add_histogram('fc1.bias', net.fc1.bias, epoch+1)\n",
    "        #tb.add_histogram('fc1.weight', net.fc1.weight, epoch+1)\n",
    "        #tb.add_histogram('fc1.weight.grad',net.fc1.weight.grad,epoch+1)\n",
    "        #tb.add_histogram('fc2.bias', net.fc2.bias, epoch+1)\n",
    "        #tb.add_histogram('fc2.weight', net.fc2.weight, epoch+1)\n",
    "        #tb.add_histogram('fc2.weight.grad',net.fc2.weight.grad,epoch+1)\n",
    "        #tb.add_histogram('fc3.bias', net.fc3.bias, epoch+1)\n",
    "        #tb.add_histogram('fc3.weight', net.fc3.weight, epoch+1)\n",
    "        #tb.add_histogram('fc3.weight.grad',net.fc3.weight.grad,epoch+1)\n",
    "        \n",
    "        \n",
    "        # Save models\n",
    "        comment = f' trg_dataset{trg_dataset_ref} batch_size={batch_size} learning_rate={learning_rate} scheduler_step_size={scheduler_step_size} scheduler_gamma={scheduler_gamma} weight_decay={weight_decay} epoch_number={epoch} accuracy={val_accuracy}'\n",
    "        torch.save(net.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/pruning_testmodel_' + date_now + '_' + time_now + '_' + comment + '_SAVE.pth')\n",
    "        pruner.export_model(model_path='/content/drive/MyDrive/KASHIKO/MODELS/pruning_testmodel_' + date_now + '_' + time_now + '_' + comment + '_EXPORT.pth', mask_path='/content/drive/MyDrive/KASHIKO/MODELS/pruning_testmodel_' + date_now + '_' + time_now + '_' + comment + '_MASK.pth')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    # At the end of the training, close the tensorboard SummaryWriter and save the model to the drive\n",
    "    tb.close()\n",
    "    print('Training Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBLN_dWoO1cL",
    "outputId": "0afe795d-75f3-404e-a82d-7dba886631f3"
   },
   "outputs": [],
   "source": [
    "pruner.export_model(model_path='/content/drive/MyDrive/KASHIKO/MODELS/last_model3_pruned.pth', mask_path='/content/drive/MyDrive/KASHIKO/MODELS/mask_last_model1_pruned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n3HwruczkyK"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'/content/drive/MyDrive/KASHIKO/MODELS/best_model3_pruned.pth', _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXyY7-5Pza23"
   },
   "source": [
    "### Test perfomance (accuracy and speed) of post-pruning post-retraining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYLH9oukLi4N"
   },
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "net3_pruned = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfuOm78ZOFGk",
    "outputId": "d88731f3-b2a8-449d-c43e-ef9f4f7083c1"
   },
   "outputs": [],
   "source": [
    "# Load pruned model\n",
    "state_dict3_pruned = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/last_model3_pruned.pth')\n",
    "net3_pruned.load_state_dict(state_dict3_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNVnGKQtza24",
    "outputId": "2ba4ff02-7f9b-427b-db68-4e369c6e4554"
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "# Set metrics to 0\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "total_sure = 0\n",
    "correct_sure = 0\n",
    "total_sure_temp = 0\n",
    "correct_sure_temp = 0\n",
    "# Define softmax function\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "# Perform a fowrad pass of the dataset through the pruned and retrained model\n",
    "# and compute performance and time to complete processing\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        net3_pruned.eval()\n",
    "        out = net3_pruned(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        predicted_soft = m(out)\n",
    "        predicted_soft_temp = m(out/temp_factor_net1)\n",
    "        if np.amax(predicted_soft.numpy()) > 0.97:\n",
    "            total_sure += labels.size(0)\n",
    "            correct_sure += (predicted == labels).sum().item()\n",
    "        if np.amax(predicted_soft_temp.numpy()) > 0.73:\n",
    "            total_sure_temp += labels.size(0)\n",
    "            correct_sure_temp += (predicted == labels).sum().item()\n",
    "        total_all += labels.size(0)\n",
    "        correct_all += (predicted == labels).sum().item()\n",
    "        \n",
    "test_accuracy_all = 100 * correct_all / total_all\n",
    "test_accuracy_sure = 100 * correct_sure / total_sure\n",
    "test_accuracy_sure_temp = 100 * correct_sure_temp / total_sure_temp\n",
    "\n",
    "print(test_accuracy_all)\n",
    "print(test_accuracy_sure)\n",
    "print(100 * total_sure/total_all)\n",
    "print(test_accuracy_sure_temp)\n",
    "print(100 * total_sure_temp/total_all)\n",
    "\n",
    "end_time = time.clock()\n",
    "print(end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of model_pruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
