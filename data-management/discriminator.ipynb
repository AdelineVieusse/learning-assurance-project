{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox10dovkAQiE"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VfIcKLF-FLk",
    "outputId": "474bd5e8-cdca-4bde-8c2e-da333c8e5f0d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2i8605HAQiH"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM6EugCD-FLk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "import os\n",
    "import torchvision.models as tmodels\n",
    "from functools import partial\n",
    "import collections\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-qpX74HAQiI"
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuUeKg6VAQiJ"
   },
   "outputs": [],
   "source": [
    "def get_mean_channels(batched_outputs):\n",
    "    channel_means = []\n",
    "    for single_output in batched_outputs:\n",
    "        channel_means.append([channel.mean() for channel in single_output])\n",
    "    return torch.tensor(channel_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi4YZIX0AQiK"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkW4imRc-FLl"
   },
   "outputs": [],
   "source": [
    "trg_dataset_ref = 1\n",
    "\n",
    "# Retrieve normalisation parameters \n",
    "\n",
    "norm_param_df = pd.read_csv('/content/drive/MyDrive/KASHIKO/DATASET/TRG_DATASET_NORM_PARAM.csv')\n",
    "\n",
    "meanR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanR\"].item()\n",
    "meanG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanG\"].item()\n",
    "meanB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"meanB\"].item()\n",
    "\n",
    "stdR = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdR\"].item()\n",
    "stdG = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdG\"].item()\n",
    "stdB = norm_param_df.loc[norm_param_df[\"Dataset\"] == str(trg_dataset_ref), \"stdB\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scrMU_PJ-FLm"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    '/content/drive/MyDrive/KASHIKO/DATASET/TRG_1_FINAL',\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnNH06eHAQiM"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 1000, 1000])\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLBbbG8wAQiN"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhSz9Y9L-FLm"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(24*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1,24*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yBIrUwF-FLn"
   },
   "outputs": [],
   "source": [
    "net1 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ7SZaqP-FLn"
   },
   "outputs": [],
   "source": [
    "state_dict1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/model_2021-05-29_12:16:11_ trg_dataset1 batch_size=100 learning_rate=0.001 scheduler_step_size=5 scheduler_gamma=1 weight_decay=0 epoch_number=14 accuracy=98.0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyHrfS8h-FLn",
    "outputId": "df047de4-5b46-44a0-f054-a2d5db7f2c67"
   },
   "outputs": [],
   "source": [
    "net1.load_state_dict(state_dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b3ZdCn8AQiQ"
   },
   "source": [
    "### Generate activation files for SVM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bytvmBA-FLo"
   },
   "outputs": [],
   "source": [
    "activations1 = collections.defaultdict(list)\n",
    "def save_activation1(name, mod, inp, out1):\n",
    "    activations1[name].append(out1.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5VgaJ4s-FLo"
   },
   "outputs": [],
   "source": [
    "for name, m in net1.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activation1, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activation1, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt2sKn67-FLo"
   },
   "outputs": [],
   "source": [
    "# Forward pass of the full dataset\n",
    "with torch.no_grad():\n",
    "  for images, labels in loader:\n",
    "    net1.eval()\n",
    "    out1 = net1(images)\n",
    "\n",
    "activations1 = {name: torch.cat(outputs, 0) for name, outputs in activations1.items()}\n",
    "torch.save(activations1,'/content/drive/MyDrive/KASHIKO/MODELS/SVM_training_activations1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl_XMPggIlYh"
   },
   "source": [
    "### Generate activation files for the In-Distribution Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUfov3cbIUQS"
   },
   "outputs": [],
   "source": [
    "\n",
    "activationsID = collections.defaultdict(list)\n",
    "def save_activationID(name, mod, inp, outID):\n",
    "    activationsID[name].append(outID.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDeCeOmKQgBQ"
   },
   "outputs": [],
   "source": [
    "ID_valset = datasets.ImageFolder('/content/drive/MyDrive/KASHIKO/DATASET/ID2',\n",
    "    transforms.Compose([transforms.ToTensor(),transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f7apZU-Q2zQ"
   },
   "outputs": [],
   "source": [
    "IDloader = torch.utils.data.DataLoader(\n",
    "        ID_valset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SdRDMXZQ8MX"
   },
   "outputs": [],
   "source": [
    "for name, m in net1.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        # partial to assign the layer name to each hook\n",
    "        m.register_forward_hook(partial(save_activationID, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activationID, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activationID, name))\n",
    "        \n",
    "with torch.no_grad():\n",
    "  for images, labels in IDloader:\n",
    "    net1.eval()\n",
    "    outID = net1(images)\n",
    "\n",
    "activationsID = {name: torch.cat(outputs, 0) for name, outputs in activationsID.items()}\n",
    "torch.save(activationsID,'/content/drive/MyDrive/KASHIKO/MODELS/SVM_testing_activationsID2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUMRQu1XQ6UA"
   },
   "source": [
    "### Generate activation files for the Out-Of-Distribution Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtVRH_XSKfO7"
   },
   "outputs": [],
   "source": [
    "OOD_valset = datasets.ImageFolder('/content/drive/MyDrive/KASHIKO/DATASET/OOD',\n",
    "    transforms.Compose([transforms.ToTensor(),transforms.Normalize((meanR, meanG, meanB), (stdR, stdG, stdB))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDSAu2J3KzLY"
   },
   "outputs": [],
   "source": [
    "OODloader = torch.utils.data.DataLoader(\n",
    "        OOD_valset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5hkSO9jK646"
   },
   "outputs": [],
   "source": [
    "#activations.clear()\n",
    "activationsOOD = collections.defaultdict(list)\n",
    "def save_activationOOD(name, mod, inp, outOOD):\n",
    "    activationsOOD[name].append(outOOD.cpu())\n",
    "    \n",
    "for name, m in net1.named_modules():\n",
    "    if type(m)==nn.Conv2d:\n",
    "        # partial to assign the layer name to each hook\n",
    "        m.register_forward_hook(partial(save_activationOOD, name))\n",
    "    elif type(m)==nn.Linear:\n",
    "        m.register_forward_hook(partial(save_activationOOD, name))\n",
    "    elif type(m)==nn.BatchNorm2d:\n",
    "        m.register_forward_hook(partial(save_activationOOD, name))\n",
    "        \n",
    "with torch.no_grad():\n",
    "  for images, labels in OODloader:\n",
    "    net1.eval()\n",
    "    outOOD = net1(images)\n",
    "\n",
    "activationsOOD = {name: torch.cat(outputs, 0) for name, outputs in activationsOOD.items()}\n",
    "torch.save(activationsOOD,'/content/drive/MyDrive/KASHIKO/MODELS/SVM_testing_activationsOOD.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyCjnGCAAQiY"
   },
   "source": [
    "### Load Activation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbaP8gy8iGkV",
    "outputId": "a06d38e2-073e-42bd-8194-057a7fc31b73"
   },
   "outputs": [],
   "source": [
    "ACT1 = collections.defaultdict(list)\n",
    "ACT1 = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/SVM_training_activations1.pt')\n",
    "\n",
    "ACTID = collections.defaultdict(list)\n",
    "ACTID = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/SVM_testing_activationsID2.pt')\n",
    "\n",
    "ACTOOD = collections.defaultdict(list)\n",
    "ACTOOD = torch.load('/content/drive/MyDrive/KASHIKO/MODELS/SVM_testing_activationsOOD.pt')\n",
    "\n",
    "# Sanity check of the size/shape of the Activation files\n",
    "for k,v in ACT1.items():\n",
    "    print (k, v.size())\n",
    "\n",
    "for k,v in ACTID.items():\n",
    "    print (k, v.size())\n",
    "\n",
    "for k,v in ACTOOD.items():\n",
    "    print (k, v.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0wA0NilkUGb"
   },
   "source": [
    "### Train SVM Model for each layer and compare performance with ID and OOD Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXAHM9sOjnqM",
    "outputId": "4be6126b-0e00-4a6b-bd50-3651b11d5a41"
   },
   "outputs": [],
   "source": [
    "# CONV1\n",
    "batched_output = get_mean_channels(ACT1['conv1'])\n",
    "model = OneClassSVM(gamma='auto', nu=0.001).fit(batched_output)\n",
    "\n",
    "id_features = get_mean_channels(ACTID['conv1'])\n",
    "ood_features = get_mean_channels(ACTOOD['conv1'])\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNG9oMeNOmVj",
    "outputId": "05c5f3ae-fcb4-40e0-ac3f-1c92b7860f0a"
   },
   "outputs": [],
   "source": [
    "# BN1\n",
    "batched_output = get_mean_channels(ACT1['bn1'])\n",
    "model = OneClassSVM(gamma='auto', nu=0.001).fit(batched_output)\n",
    "\n",
    "id_features = get_mean_channels(ACTID['bn1'])\n",
    "ood_features = get_mean_channels(ACTOOD['bn1'])\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vell6rcRVu_",
    "outputId": "b6a347d5-c33a-4886-bbf2-8088cf440375"
   },
   "outputs": [],
   "source": [
    "# CONV2\n",
    "batched_output = get_mean_channels(ACT1['conv2'])\n",
    "model = OneClassSVM(gamma='auto', nu=0.001).fit(batched_output)\n",
    "\n",
    "id_features = get_mean_channels(ACTID['conv2'])\n",
    "ood_features = get_mean_channels(ACTOOD['conv2'])\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCWgHdBymLIZ",
    "outputId": "27fc8911-dd5c-43e8-e54b-184a34db06ec"
   },
   "outputs": [],
   "source": [
    "# BN2\n",
    "batched_output = get_mean_channels(ACT1['bn2'])\n",
    "model = OneClassSVM(gamma='auto', nu=0.001).fit(batched_output)\n",
    "\n",
    "id_features = get_mean_channels(ACTID['bn2'])\n",
    "ood_features = get_mean_channels(ACTOOD['bn2'])\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABB04CsJO8-G",
    "outputId": "3f923636-996e-4bb3-c7ad-749d0bb9896d"
   },
   "outputs": [],
   "source": [
    "# FC1\n",
    "model = OneClassSVM(gamma='auto', nu = 0.001).fit(ACT1['fc1'])\n",
    "\n",
    "id_features = ACTID['fc1']\n",
    "ood_features = ACTOOD['fc1']\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2t0ZkLvmIiD",
    "outputId": "2296d6c1-1041-4249-a3f8-f1950ec12cad"
   },
   "outputs": [],
   "source": [
    "# FC2\n",
    "model = OneClassSVM(gamma='auto', nu = 0.001).fit(ACT1['fc2'])\n",
    "\n",
    "id_features = ACTID['fc2']\n",
    "ood_features = ACTOOD['fc2']\n",
    "\n",
    "data = np.vstack((id_features,ood_features))\n",
    "preds = model.predict(data)\n",
    "\n",
    "id_error = 1 - np.count_nonzero(preds[:100] == 1) / 100\n",
    "ood_error = 1 - np.count_nonzero(preds[100:] == -1) / 100\n",
    "detection_errors = (id_error + ood_error) / 2\n",
    "\n",
    "print(f'ID error : {id_error}')\n",
    "print(f'ood_error : {ood_error}')\n",
    "print(f'Detection error for layer : {detection_errors}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "discriminator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
