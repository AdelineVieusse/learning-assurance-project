{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import Augmentor\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import random\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters depending on dataset to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset_type = 'TRG_'\n",
    "current_dataset_number = '1_'\n",
    "\n",
    "# Set different seed for data augmentation depending on dataset to be prepared\n",
    "if current_dataset_number == '1_':\n",
    "    seed = 100\n",
    "elif current_dataset_number == '2_':\n",
    "    seed = 200\n",
    "elif current_dataset_number == '3_':\n",
    "    seed = 300\n",
    "else:\n",
    "    seed = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that moves a file from a directory to another\n",
    "def move_file(old_dir, old_file_name, new_dir, new_file_name):\n",
    "    old_file = os.path.join(old_dir, old_file_name)\n",
    "    new_file = os.path.join(new_dir, new_file_name)\n",
    "    shutil.copy2(old_file, new_dir)\n",
    "\n",
    "# Define a function to compute the hash of a file (except image file)\n",
    "# and compares it to the hash computed/recorded when the file was generated\n",
    "def check_hash(directory, file_name, hash_file_name):\n",
    "    print(file_name)\n",
    "    # Compute hash value\n",
    "    BLOCK_SIZE = 65536\n",
    "    file_hash = hashlib.sha512()\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        fb = f.read(BLOCK_SIZE) \n",
    "        while len(fb) > 0: \n",
    "            file_hash.update(fb)\n",
    "            fb = f.read(BLOCK_SIZE)\n",
    "        computed_hash = file_hash.hexdigest()\n",
    "    #print(computed_hash)\n",
    "    \n",
    "    # Extract stored hash value \n",
    "    hash_file_path = os.path.join(directory, hash_file_name)\n",
    "    with open(hash_file_path, \"rb\") as f:\n",
    "        stored_hash = f.read().decode(\"utf-8\")\n",
    "    #print(stored_hash)\n",
    "\n",
    "    # Compare hash values\n",
    "    hash_compare = 0\n",
    "    if computed_hash == stored_hash:\n",
    "        hash_compare = 1\n",
    "        print(\"File OK\")\n",
    "    else:\n",
    "        zip_hash_compare = 0\n",
    "        print(\"File NOK\")\n",
    "\n",
    "# Define a function to compute the hash of an image file\n",
    "# and compares it to the hash computed/recorded when the file was generated\n",
    "def check_hash_image(directory, file_name, stored_hash):\n",
    "    print(file_name)\n",
    "    # Compute hash value\n",
    "    BLOCK_SIZE = 65536\n",
    "    file_hash = hashlib.sha256()\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        fb = f.read(BLOCK_SIZE) \n",
    "        while len(fb) > 0: \n",
    "            file_hash.update(fb)\n",
    "            fb = f.read(BLOCK_SIZE)\n",
    "        computed_hash = file_hash.hexdigest()\n",
    "    \n",
    "    # Compare hash values\n",
    "    hash_compare = 0\n",
    "    if computed_hash == stored_hash:\n",
    "        hash_compare = 1\n",
    "        print(\"File OK\")\n",
    "    else:\n",
    "        zip_hash_compare = 0\n",
    "        print(\"File NOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that performs the augmentation, copies the images into the final folder and updates the csv\n",
    "def modif_save_new_image (old_dir, new_dir, modif, seed):\n",
    "    # Set variables\n",
    "    image_param_column_list = ['UUID','DATE','TIME','HOLDING_POINT', \n",
    "                               'EDGE ROTATION', 'EDGE DIST', 'EDGE ANGLE','SIDE',\n",
    "                               'MARKING', ' LIGHT', 'LONG DIST', 'LAT DIST', 'ANGLE','SHA256',\n",
    "                               'CORRECTION', 'COMMENT', 'MODIF', 'SEED', 'FLIP', \n",
    "                               'SKEW_SIDE', 'SKEW_TOP', 'ROTATE_LEFT', 'ROTATE_RIGHT', \n",
    "                               'BRIGHT', 'DARK', 'SHEAR_LEFT', 'SHEAR_RIGHT', \n",
    "                               'DISTORT', 'CONTRAST', 'ORIGINAL']\n",
    "    global combined_csv_df, combined_csv_df_frozen\n",
    "    flip, skew_side, skew_top, rotate_left, rotate_right, bright, dark, shear_left, shear_right, distort, contrast = 0,0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    # Create augmentation pipeline\n",
    "    Augmentor.Pipeline.set_seed(seed)\n",
    "    if modif == \"flip\":\n",
    "        p_flip = Augmentor.Pipeline(new_dir)\n",
    "        p_flip.flip_left_right(probability=1)\n",
    "        p_flip.process()\n",
    "        flip = 1\n",
    "    elif modif == \"skew_side\":\n",
    "        p_skew_side = Augmentor.Pipeline(new_dir)\n",
    "        p_skew_side.skew_left_right(0.5, magnitude=1)\n",
    "        p_skew_side.sample(500)\n",
    "        skew_side = 1\n",
    "    elif modif == \"skew_top\":\n",
    "        p_skew_top = Augmentor.Pipeline(new_dir)\n",
    "        p_skew_top.skew_top_bottom(0.5, magnitude=1)\n",
    "        p_skew_top.sample(500)\n",
    "        skew_top = 1    \n",
    "    elif modif == \"rotate_left\":\n",
    "        p_rotate_left = Augmentor.Pipeline(new_dir)\n",
    "        p_rotate_left.rotate(probability=0.5, max_left_rotation=5, max_right_rotation=0)\n",
    "        p_rotate_left.sample(500)\n",
    "        rotate_left = 1\n",
    "    elif modif == \"rotate_right\":\n",
    "        p_rotate_right = Augmentor.Pipeline(new_dir)\n",
    "        p_rotate_right.rotate(probability=0.5, max_left_rotation=0, max_right_rotation=5)\n",
    "        p_rotate_right.sample(500)\n",
    "        rotate_right = 1\n",
    "    elif modif == \"bright\":\n",
    "        p_bright = Augmentor.Pipeline(new_dir)\n",
    "        p_bright.random_brightness(probability = 0.5, min_factor = 1.05, max_factor = 1.2)\n",
    "        p_bright.sample(500)\n",
    "        bright = 1\n",
    "    elif modif == \"dark\":\n",
    "        p_dark = Augmentor.Pipeline(new_dir)\n",
    "        p_dark.random_brightness(probability = 0.5, min_factor = 0.8, max_factor = 0.95)\n",
    "        p_dark.sample(500)\n",
    "        dark = 1\n",
    "    elif modif == \"shear_left\":\n",
    "        p_shear_left = Augmentor.Pipeline(new_dir)\n",
    "        p_shear_left.shear(probability = 0.5, max_shear_left = 10, max_shear_right = 0.001)\n",
    "        p_shear_left.sample(500)\n",
    "        shear_left = 1\n",
    "    elif modif == \"shear_right\":\n",
    "        p_shear_right = Augmentor.Pipeline(new_dir)\n",
    "        p_shear_right.shear(probability = 0.5, max_shear_left = 0.001, max_shear_right = 10)\n",
    "        p_shear_right.sample(500)\n",
    "        shear_right = 1\n",
    "    elif modif == \"distort\":\n",
    "        p_distort = Augmentor.Pipeline(new_dir)\n",
    "        p_distort.random_distortion(probability = 0.5, grid_width = 10, grid_height = 10, magnitude = 2)\n",
    "        p_distort.sample(500)\n",
    "        distort = 1        \n",
    "    elif modif == \"contrast\":\n",
    "        p_contrast = Augmentor.Pipeline(new_dir)\n",
    "        p_contrast.random_contrast(probability = 0.5, min_factor = 0.90, max_factor = 0.95)\n",
    "        p_contrast.sample(500)\n",
    "        contrast = 1\n",
    "        \n",
    "    for file in os.listdir(old_dir):\n",
    "        # Find new file name/UUID and original file name/UUID\n",
    "        long_file_name = os.fsdecode(file)\n",
    "        split_file_name = long_file_name.split('.jpg_') \n",
    "        original_file_name = split_file_name[0].split('DATASET_original_')[0]\n",
    "        new_file_name = split_file_name[1]\n",
    "        # Move file to new directory\n",
    "        old_file = os.path.join(old_dir, long_file_name)\n",
    "        new_file = os.path.join(new_dir, new_file_name)\n",
    "        os.rename(old_file, new_file)\n",
    "        # Create temporary df to store new image parameters\n",
    "        # Find new file name/UUID and original file name/UUID\n",
    "        original_UUID = original_file_name.split('.jpg')[0].split('_')[2]\n",
    "        print(original_UUID)\n",
    "        new_UUID = new_file_name.split('.jpg')[0]\n",
    "        print(new_UUID)\n",
    "        # Record current date/time\n",
    "        date_now = str(date.today())\n",
    "        time_now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        # Compute hash for new image\n",
    "        file_hash = hashlib.sha256()\n",
    "        BLOCK_SIZE = 65536\n",
    "        with open(new_file, \"rb\") as f:\n",
    "            fb = f.read(BLOCK_SIZE) \n",
    "            while len(fb) > 0: \n",
    "                file_hash.update(fb)\n",
    "                fb = f.read(BLOCK_SIZE)\n",
    "            jpg_hash = file_hash.hexdigest()\n",
    "        # Record parameters in temporary dataframe    \n",
    "        temp_df = pd.DataFrame([[new_UUID, date_now, time_now, \n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'HOLDING_POINT'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'EDGE ROTATION'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'EDGE DIST'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'EDGE ANGLE'].item(),\n",
    "                                 abs(combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SIDE'].item()-1) if flip == 1\n",
    "                                 else combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SIDE'].item(), \n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'MARKING'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, ' LIGHT'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'LONG DIST'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'LAT DIST'].item(),\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'ANGLE'].item(),\n",
    "                                 jpg_hash, 0, 'NA', modif, seed,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'FLIP'].item()+flip,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SKEW_SIDE'].item()+skew_side,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SKEW_TOP'].item()+skew_top,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'ROTATE_LEFT'].item()+rotate_left,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'ROTATE_RIGHT'].item()+rotate_right,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'BRIGHT'].item()+bright,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'DARK'].item()+dark,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SHEAR_LEFT'].item()+shear_left,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'SHEAR_RIGHT'].item()+shear_right,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'DISTORT'].item()+distort,\n",
    "                                 combined_csv_df.loc[combined_csv_df['UUID'] == original_UUID, 'CONTRAST'].item()+contrast,\n",
    "                                 original_UUID]], columns=image_param_column_list)\n",
    "        # Append temporary dataframe to image parameter dataframe\n",
    "        combined_csv_df = combined_csv_df.append(temp_df)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve .zip file and corresponding hash file from Downloads folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads_directory = 'C:/Users/adeli/Downloads/'\n",
    "local_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/'\n",
    "dataset_file_name = 'dataset.zip'\n",
    "dataset_hash_file_name = 'dataset_hash.txt'\n",
    "\n",
    "move_file(downloads_directory, dataset_file_name, local_directory, dataset_file_name)\n",
    "move_file(downloads_directory, dataset_hash_file_name, local_directory, dataset_hash_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check .zip file for corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/'\n",
    "dataset_file_name = 'dataset.zip'\n",
    "dataset_hash_file_name = 'dataset_hash.txt'\n",
    "check_hash(local_directory, dataset_file_name, dataset_hash_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip .zip file to TEMP directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('DATASET/' + current_dataset_type + 'ORIGINAL/dataset.zip') as dataset_zipfile:\n",
    "    dataset_zipfile.extractall(path='DATASET/' + current_dataset_type + 'ORIGINAL/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check .csv files for corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/dataset/'\n",
    "\n",
    "for file in os.listdir(temp_directory):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_file_name = file\n",
    "        csv_hash_file_name = csv_file_name.split('.csv')[0] + '_hash.txt'\n",
    "        check_hash(temp_directory, csv_file_name, csv_hash_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all .csv files into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/dataset/'\n",
    "combined_csv_column_list = ['UUID','DATE','TIME','HOLDING_POINT', \n",
    "                               'EDGE ROTATION', 'EDGE DIST', 'EDGE ANGLE','SIDE',\n",
    "                               'MARKING', ' LIGHT', 'LONG DIST', 'LAT DIST', 'ANGLE','SHA256',\n",
    "                               'CORRECTION', 'COMMENT', 'MODIF', 'SEED', 'FLIP', \n",
    "                               'SKEW_SIDE', 'SKEW_TOP', 'ROTATE_LEFT', 'ROTATE_RIGHT', \n",
    "                               'BRIGHT', 'DARK', 'SHEAR_LEFT', 'SHEAR_RIGHT', \n",
    "                               'DISTORT', 'CONTRAST', 'ORIGINAL']\n",
    "combined_csv_df = pd.DataFrame(columns=combined_csv_column_list)\n",
    "\n",
    "for file in os.listdir(temp_directory):\n",
    "    if file.endswith('.csv'):\n",
    "        new_df = pd.read_csv(temp_directory+file)\n",
    "        combined_csv_df = pd.concat([combined_csv_df, new_df])\n",
    "combined_csv_df.to_csv( 'DATASET/' + current_dataset_type + 'ORIGINAL/combined_csv.csv', index=False, encoding='utf-8-sig')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform corrections manually in combined_csv.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all original images in final dataset folder and check all images for corruption\n",
    "\n",
    "# Set directory names\n",
    "temp_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/'\n",
    "temp_holdingpoint_image_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/dataset/holdingpoint/'\n",
    "temp_noholdingpoint_image_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/dataset/noholdingpoint/'\n",
    "\n",
    "final_holdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/holdingpoint/'\n",
    "final_noholdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/noholdingpoint/'\n",
    "\n",
    "# Open .csv file containing all images information\n",
    "combined_csv_df = pd.read_csv(temp_directory + \"combined_csv.csv\")\n",
    "\n",
    "holdingpoint_image_counter = 0\n",
    "noholdingpoint_image_counter = 0\n",
    "\n",
    "# For each image listed in the .csv file, move the image to the relevant folder and check its hash\n",
    "for index, row in combined_csv_df.iterrows():\n",
    "    if row['UUID'] != 'NA':\n",
    "        if row['HOLDING_POINT'] == 0:\n",
    "            image_name = str(row['UUID']) + '.jpg'\n",
    "            if os.path.isfile(temp_noholdingpoint_image_directory+image_name):\n",
    "                move_file(temp_noholdingpoint_image_directory, image_name, final_noholdingpoint_image_directory, image_name)\n",
    "                check_hash_image(final_noholdingpoint_image_directory, image_name, row['SHA256'])\n",
    "                noholdingpoint_image_counter += 1\n",
    "            else:\n",
    "                print(\"Missing:\",image_name)\n",
    "        elif row['HOLDING_POINT'] == 1:\n",
    "            image_name = str(row['UUID']) + '.jpg'\n",
    "            if os.path.isfile(temp_holdingpoint_image_directory+image_name):\n",
    "                move_file(temp_holdingpoint_image_directory, image_name, final_holdingpoint_image_directory, image_name)\n",
    "                check_hash_image(final_holdingpoint_image_directory, image_name, row['SHA256'])\n",
    "                holdingpoint_image_counter += 1\n",
    "            else:\n",
    "                print(\"Missing:\",image_name)\n",
    "\n",
    "print(\"images moved to holdingpoint folder\")\n",
    "print(holdingpoint_image_counter)\n",
    "print(\"images moved to holdingpoint folder\")\n",
    "print(noholdingpoint_image_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Data Augmentation\n",
    "\n",
    "# Set directory names\n",
    "temp_directory = 'DATASET/' + current_dataset_type + 'ORIGINAL/'\n",
    "combined_csv_df = pd.read_csv(temp_directory + \"combined_csv.csv\")\n",
    "\n",
    "output_holdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/holdingpoint/output/'\n",
    "final_holdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/holdingpoint/'\n",
    "output_noholdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/noholdingpoint/output/'\n",
    "final_noholdingpoint_image_directory = 'DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/noholdingpoint/'\n",
    "\n",
    "# Set list of modifications to be performed\n",
    "list_modif = list(['skew_side', 'skew_top', 'rotate_left', 'rotate_right', 'bright', 'dark', \n",
    "                   'shear_left', 'shear_right', 'distort', 'contrast'])\n",
    "\n",
    "# Shuffle the list of modifications according to the seed set at the beginning\n",
    "random.seed(seed)\n",
    "random.shuffle(list_modif)\n",
    "\n",
    "# For all images in the holdingpoint folder, flip the images and save them in the final holdingpoint folder\n",
    "modif_save_new_image (output_holdingpoint_image_directory, final_holdingpoint_image_directory, 'flip', seed)\n",
    "\n",
    "# For all images in the holdingpoint folder, apply all the other modifications in the order set by the seed\n",
    "# and save them in the final holdingpoint folder\n",
    "for modif in list_modif:\n",
    "    modif_save_new_image (output_holdingpoint_image_directory, final_holdingpoint_image_directory, modif, seed)\n",
    "\n",
    "# For all images in the noholdingpoint folder, flip the images and save them in the final noholdingpoint folder\n",
    "modif_save_new_image (output_noholdingpoint_image_directory, final_noholdingpoint_image_directory, 'flip', seed)\n",
    "\n",
    "# For all images in the noholdingpoint folder, apply all the other modifications in the order set by the seed\n",
    "# and save them in the final noholdingpoint folder\n",
    "for modif in list_modif:\n",
    "    modif_save_new_image (output_noholdingpoint_image_directory, final_noholdingpoint_image_directory, modif, seed)\n",
    "\n",
    "# Save a record of all the modification performed in a .csv file\n",
    "combined_csv_df.to_csv('DATASET/' + current_dataset_type + current_dataset_number + 'FINAL/augmented_csv.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
